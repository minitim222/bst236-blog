<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>arXiv Feed · Tim's Coding Blog</title>
    <link rel="stylesheet" href="styles.css" />
  </head>
  <body>
    <header class="site-header">
      <div class="container header-content">
        <div class="brand">
          <span class="brand-mark">&lt;/&gt;</span>
          <span class="brand-text">Tim's Coding Blog</span>
        </div>
        <nav class="nav">
          <a href="index.html" class="nav-link">Home</a>
          <a href="pacman.html" class="nav-link">Pac-Man Game</a>
          <a href="arxiv.html" class="nav-link active">arXiv Feed</a>
        </nav>
      </div>
    </header>

    <main class="page-main">
      <section class="container page-header">
        <div
          style="
            display: flex;
            justify-content: space-between;
            gap: 1rem;
            align-items: flex-end;
          "
        >
          <div>
            <div class="pill">
              <span class="pill-dot"></span>
              <span>Auto-updating feed</span>
            </div>
            <h1>Latest arXiv Papers</h1>
            <p>
              A nightly-updated list of recent arXiv papers based on my chosen
              keywords.
            </p>
          </div>
          <div
            style="
              text-align: right;
              font-size: 0.78rem;
              color: #9ca3af;
              line-height: 1.5;
            "
          >
            <div>Last updated: <span id="last-updated">2026-02-19 02:08 UTC</span></div>
            <div>Query: <span>statistics, causal+inference, machine+learning</span></div>
          </div>
        </div>
      </section>

      <section class="container">
        <div
          style="
            border-radius: 18px;
            border: 1px solid rgba(148, 163, 184, 0.55);
            background: radial-gradient(circle at top, #020617, #020617);
            box-shadow: 0 18px 45px rgba(15, 23, 42, 1);
            padding: 1.1rem 1.2rem 1.4rem;
          "
        >
          <div
            style="
              display: flex;
              justify-content: space-between;
              align-items: center;
              margin-bottom: 0.7rem;
              gap: 0.8rem;
            "
          >
            <h2
              style="
                margin: 0;
                font-size: 1.1rem;
                display: flex;
                align-items: center;
                gap: 0.35rem;
              "
            >
              <span>Recent papers</span>
            </h2>
            <span
              style="
                font-size: 0.8rem;
                color: #9ca3af;
                border-radius: 999px;
                padding: 0.18rem 0.6rem;
                border: 1px solid rgba(148, 163, 184, 0.45);
              "
              >Generated automatically by GitHub Actions</span
            >
          </div>

          <div class="paper-list">
            <article class="paper-card">
  <h3 class="paper-title">
    <a href="https://arxiv.org/pdf/2602.16709v1" target="_blank" rel="noopener noreferrer">
      Knowledge-Embedded Latent Projection for Robust Representation Learning
    </a>
  </h3>
  <p class="paper-meta">
    <span class="paper-authors">Weijing Tang, Ming Yuan, Zongqi Xia, Tianxi Cai</span>
    <span class="paper-dot">•</span>
    <span class="paper-date">2026-02-18</span>
  </p>
  <p class="paper-abstract">Latent space models are widely used for analyzing high-dimensional discrete data matrices, such as patient-feature matrices in electronic health records (EHRs), by capturing complex dependence structures through low-dimensional embeddings. However, estimation becomes challenging in the imbalanced regime, where one matrix dimension is much larger than the other. In EHR applications, cohort sizes are often limited by disease prevalence or data availability, whereas the feature space remains extremely large due to the breadth of medical coding system. Motivated by the increasing availability of external semantic embeddings, such as pre-trained embeddings of clinical concepts in EHRs, we propose a knowledge-embedded latent projection model that leverages semantic side information to regularize representation learning. Specifically, we model column embeddings as smooth functions of semantic embeddings via a mapping in a reproducing kernel Hilbert space. We develop a computationally efficient two-step estimation procedure that combines semantically guided subspace construction via kernel principal component analysis with scalable projected gradient descent. We establish estimation error bounds that characterize the trade-off between statistical error and approximation error induced by the kernel projection. Furthermore, we provide local convergence guarantees for our non-convex optimization procedure. Extensive simulation studies and a real-world EHR application demonstrate the effectiveness of the proposed method.</p>
  <a class="paper-link" href="https://arxiv.org/pdf/2602.16709v1" target="_blank" rel="noopener noreferrer">
    View PDF →
  </a>
</article>

<article class="paper-card">
  <h3 class="paper-title">
    <a href="https://arxiv.org/pdf/2602.16705v1" target="_blank" rel="noopener noreferrer">
      Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation
    </a>
  </h3>
  <p class="paper-meta">
    <span class="paper-authors">Runpei Dong, Ziyan Li, Xialin He, Saurabh Gupta</span>
    <span class="paper-dot">•</span>
    <span class="paper-date">2026-02-18</span>
  </p>
  <p class="paper-abstract">Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.</p>
  <a class="paper-link" href="https://arxiv.org/pdf/2602.16705v1" target="_blank" rel="noopener noreferrer">
    View PDF →
  </a>
</article>

<article class="paper-card">
  <h3 class="paper-title">
    <a href="https://arxiv.org/pdf/2602.16698v1" target="_blank" rel="noopener noreferrer">
      Causality is Key for Interpretability Claims to Generalise
    </a>
  </h3>
  <p class="paper-meta">
    <span class="paper-authors">Shruti Joshi, Aaron Mueller, David Klindt, Wieland Brendel, Patrik Reizinger, Dhanya Sridhar</span>
    <span class="paper-dot">•</span>
    <span class="paper-date">2026-02-18</span>
  </p>
  <p class="paper-abstract">Interpretability research on large language models (LLMs) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal interpretations that outrun the evidence. Our position is that causal inference specifies what constitutes a valid mapping from model activations to invariant high-level structures, the data or assumptions needed to achieve it, and the inferences it can support. Specifically, Pearl&#x27;s causal hierarchy clarifies what an interpretability study can justify. Observations establish associations between model behaviour and internal components. Interventions (e.g., ablations or activation patching) support claims how these edits affect a behavioural metric (\eg, average change in token probabilities) over a set of prompts. However, counterfactual claims -- i.e., asking what the model output would have been for the same prompt under an unobserved intervention -- remain largely unverifiable without controlled supervision. We show how causal representation learning (CRL) operationalises this hierarchy, specifying which variables are recoverable from activations and under what assumptions. Together, these motivate a diagnostic framework that helps practitioners select methods and evaluations matching claims to evidence such that findings generalise.</p>
  <a class="paper-link" href="https://arxiv.org/pdf/2602.16698v1" target="_blank" rel="noopener noreferrer">
    View PDF →
  </a>
</article>

<article class="paper-card">
  <h3 class="paper-title">
    <a href="https://arxiv.org/pdf/2602.16697v1" target="_blank" rel="noopener noreferrer">
      Protecting the Undeleted in Machine Unlearning
    </a>
  </h3>
  <p class="paper-meta">
    <span class="paper-authors">Aloni Cohen, Refael Kohen, Kobbi Nissim, Uri Stemmer</span>
    <span class="paper-dot">•</span>
    <span class="paper-date">2026-02-18</span>
  </p>
  <p class="paper-abstract">Machine unlearning aims to remove specific data points from a trained model, often striving to emulate &quot;perfect retraining&quot;, i.e., producing the model that would have been obtained had the deleted data never been included. We demonstrate that this approach, and security definitions that enable it, carry significant privacy risks for the remaining (undeleted) data points. We present a reconstruction attack showing that for certain tasks, which can be computed securely without deletions, a mechanism adhering to perfect retraining allows an adversary controlling merely $ω(1)$ data points to reconstruct almost the entire dataset merely by issuing deletion requests. We survey existing definitions for machine unlearning, showing they are either susceptible to such attacks or too restrictive to support basic functionalities like exact summation. To address this problem, we propose a new security definition that specifically safeguards undeleted data against leakage caused by the deletion of other points. We show that our definition permits several essential functionalities, such as bulletin boards, summations, and statistical learning.</p>
  <a class="paper-link" href="https://arxiv.org/pdf/2602.16697v1" target="_blank" rel="noopener noreferrer">
    View PDF →
  </a>
</article>

<article class="paper-card">
  <h3 class="paper-title">
    <a href="https://arxiv.org/pdf/2602.16696v1" target="_blank" rel="noopener noreferrer">
      Parameter-free representations outperform single-cell foundation models on downstream benchmarks
    </a>
  </h3>
  <p class="paper-meta">
    <span class="paper-authors">Huan Souza, Pankaj Mehta</span>
    <span class="paper-dot">•</span>
    <span class="paper-date">2026-02-18</span>
  </p>
  <p class="paper-abstract">Single-cell RNA sequencing (scRNA-seq) data exhibit strong and reproducible statistical structure. This has motivated the development of large-scale foundation models, such as TranscriptFormer, that use transformer-based architectures to learn a generative model for gene expression by embedding genes into a latent vector space. These embeddings have been used to obtain state-of-the-art (SOTA) performance on downstream tasks such as cell-type classification, disease-state prediction, and cross-species learning. Here, we ask whether similar performance can be achieved without utilizing computationally intensive deep learning-based representations. Using simple, interpretable pipelines that rely on careful normalization and linear methods, we obtain SOTA or near SOTA performance across multiple benchmarks commonly used to evaluate single-cell foundation models, including outperforming foundation models on out-of-distribution tasks involving novel cell types and organisms absent from the training data. Our findings highlight the need for rigorous benchmarking and suggest that the biology of cell identity can be captured by simple linear representations of single cell gene expression data.</p>
  <a class="paper-link" href="https://arxiv.org/pdf/2602.16696v1" target="_blank" rel="noopener noreferrer">
    View PDF →
  </a>
</article>

<article class="paper-card">
  <h3 class="paper-title">
    <a href="https://arxiv.org/pdf/2602.16690v1" target="_blank" rel="noopener noreferrer">
      Synthetic-Powered Multiple Testing with FDR Control
    </a>
  </h3>
  <p class="paper-meta">
    <span class="paper-authors">Yonghoon Lee, Meshi Bashari, Edgar Dobriban, Yaniv Romano</span>
    <span class="paper-dot">•</span>
    <span class="paper-date">2026-02-18</span>
  </p>
  <p class="paper-abstract">Multiple hypothesis testing with false discovery rate (FDR) control is a fundamental problem in statistical inference, with broad applications in genomics, drug screening, and outlier detection. In many such settings, researchers may have access not only to real experimental observations but also to auxiliary or synthetic data -- from past, related experiments or generated by generative models -- that can provide additional evidence about the hypotheses of interest. We introduce SynthBH, a synthetic-powered multiple testing procedure that safely leverages such synthetic data. We prove that SynthBH guarantees finite-sample, distribution-free FDR control under a mild PRDS-type positive dependence condition, without requiring the pooled-data p-values to be valid under the null. The proposed method adapts to the (unknown) quality of the synthetic data: it enhances the sample efficiency and may boost the power when synthetic data are of high quality, while controlling the FDR at a user-specified level regardless of their quality. We demonstrate the empirical performance of SynthBH on tabular outlier detection benchmarks and on genomic analyses of drug-cancer sensitivity associations, and further study its properties through controlled experiments on simulated data.</p>
  <a class="paper-link" href="https://arxiv.org/pdf/2602.16690v1" target="_blank" rel="noopener noreferrer">
    View PDF →
  </a>
</article>

<article class="paper-card">
  <h3 class="paper-title">
    <a href="https://arxiv.org/pdf/2602.16689v1" target="_blank" rel="noopener noreferrer">
      Are Object-Centric Representations Better At Compositional Generalization?
    </a>
  </h3>
  <p class="paper-meta">
    <span class="paper-authors">Ferdinand Kapl, Amir Mohammad Karimi Mamaghan, Maximilian Seitzer, Karl Henrik Johansson, Carsten Marr, Stefan Bauer, Andrea Dittadi</span>
    <span class="paper-dot">•</span>
    <span class="paper-date">2026-02-18</span>
  </p>
  <p class="paper-abstract">Compositional generalization, the ability to reason about novel combinations of familiar concepts, is fundamental to human cognition and a critical challenge for machine learning. Object-centric (OC) representations, which encode a scene as a set of objects, are often argued to support such generalization, but systematic evidence in visually rich settings is limited. We introduce a Visual Question Answering benchmark across three controlled visual worlds (CLEVRTex, Super-CLEVR, and MOVi-C) to measure how well vision encoders, with and without object-centric biases, generalize to unseen combinations of object properties. To ensure a fair and comprehensive comparison, we carefully account for training data diversity, sample size, representation size, downstream model capacity, and compute. We use DINOv2 and SigLIP2, two widely used vision encoders, as the foundation models and their OC counterparts. Our key findings reveal that (1) OC approaches are superior in harder compositional generalization settings; (2) original dense representations surpass OC only on easier settings and typically require substantially more downstream compute; and (3) OC models are more sample efficient, achieving stronger generalization with fewer images, whereas dense encoders catch up or surpass them only with sufficient data and diversity. Overall, object-centric representations offer stronger compositional generalization when any one of dataset size, training data diversity, or downstream compute is constrained.</p>
  <a class="paper-link" href="https://arxiv.org/pdf/2602.16689v1" target="_blank" rel="noopener noreferrer">
    View PDF →
  </a>
</article>

<article class="paper-card">
  <h3 class="paper-title">
    <a href="https://arxiv.org/pdf/2602.16665v1" target="_blank" rel="noopener noreferrer">
      Optimizing p-spin models through hypergraph neural networks and deep reinforcement learning
    </a>
  </h3>
  <p class="paper-meta">
    <span class="paper-authors">Li Zeng, Mutian Shen, Tianle Pu, Zohar Nussinov, Qing Feng, Chao Chen, Zhong Liu, Changjun Fan</span>
    <span class="paper-dot">•</span>
    <span class="paper-date">2026-02-18</span>
  </p>
  <p class="paper-abstract">p-spin glasses, characterized by frustrated many-body interactions beyond the conventional pairwise case (p&gt;2), are prototypical disordered systems whose ground-state search is NP-hard and computationally prohibitive for large instances. Solving this problem is not only fundamental for understanding high-order disorder, structural glasses, and topological phases, but also central to a wide spectrum of hard combinatorial optimization tasks. Despite decades of progress, there still lacks an efficient and scalable solver for generic large-scale p-spin models. Here we introduce PLANCK, a physics-inspired deep reinforcement learning framework built on hypergraph neural networks. PLANCK directly optimizes arbitrary high-order interactions, and systematically exploits gauge symmetry throughout both training and inference. Trained exclusively on small synthetic instances, PLANCK exhibits strong zero-shot generalization to systems orders of magnitude larger, and consistently outperforms state-of-the-art thermal annealing methods across all tested structural topologies and coupling distributions. Moreover, without any modification, PLANCK achieves near-optimal solutions for a broad class of NP-hard combinatorial problems, including random k-XORSAT, hypergraph max-cut, and conventional max-cut. The presented framework provides a physics-inspired algorithmic paradigm that bridges statistical mechanics and reinforcement learning. The symmetry-aware design not only advances the tractable frontiers of high-order disordered systems, but also opens a promising avenue for machine-learning-based solvers to tackle previously intractable combinatorial optimization challenges.</p>
  <a class="paper-link" href="https://arxiv.org/pdf/2602.16665v1" target="_blank" rel="noopener noreferrer">
    View PDF →
  </a>
</article>

<article class="paper-card">
  <h3 class="paper-title">
    <a href="https://arxiv.org/pdf/2602.16659v1" target="_blank" rel="noopener noreferrer">
      Updated Constraints on Infrared Cutoff Models and Implications for Large-Scale CMB Anomalies
    </a>
  </h3>
  <p class="paper-meta">
    <span class="paper-authors">Ujjwal Upadhyay, Yashi Tiwari, Tarun Souradeep</span>
    <span class="paper-dot">•</span>
    <span class="paper-date">2026-02-18</span>
  </p>
  <p class="paper-abstract">The nearly scale-invariant primordial power spectrum provides the standard initial conditions for cosmological perturbations. However, the largest scales remain only weakly constrained by CMB observations, leaving room for deviations such as an infrared (IR) cut-off. This possibility is further motivated by the persistence of large-scale CMB anomalies, most notably the low quadrupole power. In this work, we revisit several broad classes of phenomenologically motivated IR cut-off scenarios using parametrised functional forms of the primordial power spectrum. We confront these models with the latest CMB, BAO, and supernova data and derive updated constraints on the cut-off scale and associated features. Our results remain consistent with earlier studies, showing that although such models suppress power at low multipoles, the improvement in fit is marginal and does not overcome the associated parameter penalties. We therefore find no statistically significant evidence favouring IR cut-off models over the standard power-law spectrum with current data. We further explore the interplay between IR cut-off features and a possible increase in the reionisation optical depth, motivated by the recent CMB-BAO tension highlighted by DESI DR2 within the $Λ$CDM framework. We show that the additional freedom introduced by large-scale suppression is generally insufficient to support a substantial increase in optical depth, owing to the weak statistical preference for suppressed large-scale temperature power. Finally, we examine the implications of IR cut-off models for large-scale CMB anomalies by analysing the corresponding anomaly statistics within a Bayesian framework.</p>
  <a class="paper-link" href="https://arxiv.org/pdf/2602.16659v1" target="_blank" rel="noopener noreferrer">
    View PDF →
  </a>
</article>

<article class="paper-card">
  <h3 class="paper-title">
    <a href="https://arxiv.org/pdf/2602.16651v1" target="_blank" rel="noopener noreferrer">
      Interpreting the HI 21-cm cosmology maps through Largest Cluster Statistics III: Impact of the lightcone effect
    </a>
  </h3>
  <p class="paper-meta">
    <span class="paper-authors">Hemanth Potluri, Manas Mohit Dosibhatla, Leon Noble, Chandra Shekhar Murmu, Suman Majumdar, Samit Kumar Pal, Saswata Dasgupta, Satadru Bag, Abhirup Datta</span>
    <span class="paper-dot">•</span>
    <span class="paper-date">2026-02-18</span>
  </p>
  <p class="paper-abstract">The redshifted 21-cm signal emitted by neutral Hydrogen (HI) is a promising probe to understand the evolution of the topology of ionized regions during the Epoch of Reionization (EoR). The topology of ionized regions allows us to infer the nature and properties of ionizing sources, i.e., early galaxies and AGNs. Traditional Fourier statistics, such as the power spectrum, help us quantify the strength of fluctuations in this field at different length scales but do not preserve its phase information. Analyzing the 21-cm brightness temperature field in the image domain retains its non-Gaussian characteristics and morphological information. One such approach is to track the coalescence of multiple ionized regions to form one contiguous ionized region spanning the universe. This is referred to as percolation, and its onset is quantified by a sharp rise in the value of the Largest Cluster Statistic (LCS) approaching unity. In this work, we carry out a percolation analysis of 21-cm brightness temperature fields by studying the redshift evolution of the LCS along a lightcone to distinguish between several simulated reionization scenarios. We have extended previous results on reionization model comparison from the analysis of coeval 21-cm maps to understand how the lightcone effect biases the observed percolation behavior and affects the distinguishability of the source models. We estimate the LCS of subvolumes of different sizes in the 21-cm lightcone maps and study their redshift evolution for different reionization scenarios using a moving volume approach. We find that the percolation transition inferred from a lightcone approaches that from the coeval box as we increase the bandwidth of the moving volume in all but one reionization scenario.</p>
  <a class="paper-link" href="https://arxiv.org/pdf/2602.16651v1" target="_blank" rel="noopener noreferrer">
    View PDF →
  </a>
</article>

<article class="paper-card">
  <h3 class="paper-title">
    <a href="https://arxiv.org/pdf/2602.16643v1" target="_blank" rel="noopener noreferrer">
      Factorization Machine with Quadratic-Optimization Annealing for RNA Inverse Folding and Evaluation of Binary-Integer Encoding and Nucleotide Assignment
    </a>
  </h3>
  <p class="paper-meta">
    <span class="paper-authors">Shuta Kikuchi, Shu Tanaka</span>
    <span class="paper-dot">•</span>
    <span class="paper-date">2026-02-18</span>
  </p>
  <p class="paper-abstract">The RNA inverse folding problem aims to identify nucleotide sequences that preferentially adopt a given target secondary structure. While various heuristic and machine learning-based approaches have been proposed, many require a large number of sequence evaluations, which limits their applicability when experimental validation is costly. We propose a method to solve the problem using a factorization machine with quadratic-optimization annealing (FMQA). FMQA is a discrete black-box optimization method reported to obtain high-quality solutions with a limited number of evaluations. Applying FMQA to the problem requires converting nucleotides into binary variables. However, the influence of integer-to-nucleotide assignments and binary-integer encoding on the performance of FMQA has not been thoroughly investigated, even though such choices determine the structure of the surrogate model and the search landscape, and thus can directly affect solution quality. Therefore, this study aims both to establish a novel FMQA framework for RNA inverse folding and to analyze the effects of these assignments and encoding methods. We evaluated all 24 possible assignments of the four nucleotides to the ordered integers (0-3), in combination with four binary-integer encoding methods. Our results demonstrated that one-hot and domain-wall encodings outperform binary and unary encodings in terms of the normalized ensemble defect value. In domain-wall encoding, nucleotides assigned to the boundary integers (0 and 3) appeared with higher frequency. In the RNA inverse folding problem, assigning guanine and cytosine to these boundary integers promoted their enrichment in stem regions, which led to more thermodynamically stable secondary structures than those obtained with one-hot encoding.</p>
  <a class="paper-link" href="https://arxiv.org/pdf/2602.16643v1" target="_blank" rel="noopener noreferrer">
    View PDF →
  </a>
</article>

<article class="paper-card">
  <h3 class="paper-title">
    <a href="https://arxiv.org/pdf/2602.16634v1" target="_blank" rel="noopener noreferrer">
      Enhanced Diffusion Sampling: Efficient Rare Event Sampling and Free Energy Calculation with Diffusion Models
    </a>
  </h3>
  <p class="paper-meta">
    <span class="paper-authors">Yu Xie, Ludwig Winkler, Lixin Sun, Sarah Lewis, Adam E. Foster, José Jiménez Luna, Tim Hempel, Michael Gastegger, Yaoyi Chen, Iryna Zaporozhets, Cecilia Clementi, Christopher M. Bishop, Frank Noé</span>
    <span class="paper-dot">•</span>
    <span class="paper-date">2026-02-18</span>
  </p>
  <p class="paper-abstract">The rare-event sampling problem has long been the central limiting factor in molecular dynamics (MD), especially in biomolecular simulation. Recently, diffusion models such as BioEmu have emerged as powerful equilibrium samplers that generate independent samples from complex molecular distributions, eliminating the cost of sampling rare transition events. However, a sampling problem remains when computing observables that rely on states which are rare in equilibrium, for example folding free energies. Here, we introduce enhanced diffusion sampling, enabling efficient exploration of rare-event regions while preserving unbiased thermodynamic estimators. The key idea is to perform quantitatively accurate steering protocols to generate biased ensembles and subsequently recover equilibrium statistics via exact reweighting. We instantiate our framework in three algorithms: UmbrellaDiff (umbrella sampling with diffusion models), $Δ$G-Diff (free-energy differences via tilted ensembles), and MetaDiff (a batchwise analogue for metadynamics). Across toy systems, protein folding landscapes and folding free energies, our methods achieve fast, accurate, and scalable estimation of equilibrium properties within GPU-minutes to hours per system -- closing the rare-event sampling gap that remained after the advent of diffusion-model equilibrium samplers.</p>
  <a class="paper-link" href="https://arxiv.org/pdf/2602.16634v1" target="_blank" rel="noopener noreferrer">
    View PDF →
  </a>
</article>

<article class="paper-card">
  <h3 class="paper-title">
    <a href="https://arxiv.org/pdf/2602.16632v1" target="_blank" rel="noopener noreferrer">
      Understanding the influence of yttrium on the dominant twinning mode and local mechanical field evolution in extruded Mg-Y alloys
    </a>
  </h3>
  <p class="paper-meta">
    <span class="paper-authors">Chaitali Patil, Qianying Shi, Abhishek Kumar, Veera Sundararaghavan, John Allison</span>
    <span class="paper-dot">•</span>
    <span class="paper-date">2026-02-18</span>
  </p>
  <p class="paper-abstract">Twinning is a primary deformation mechanism in Mg alloys. This study focuses on tension twins during uniaxial compression of Mg-Y alloys, with three key aspects: the orientation specificity of twin grains, the relative evolution of CRSS with increasing Y content, and the local stress and strain evolution at twin sites. Experimental characterization and crystal plasticity modeling were performed. In Mg-7wt.%Y, TT2-{112-1} tension twins were observed in addition to the common TT1-{101-2} twins. Increasing Y suppressed TT1 formation while promoting TT2 activity. A previously unreported group of crystallographic orientations with a higher global Schmid factor for &lt;c+a&gt; slip was identified, which exhibited TT1 twinning with increasing compression strain. To elucidate Y effects on twin activity and local mechanical fields, both TT1 and TT2 tension twin modes were incorporated into PRISMS-Plasticity, an open-source, finite element-based crystal plasticity solver. Four binary Mg-Y alloys were modeled under compression, and statistical analysis was conducted to correlate initial orientations, stress-strain distributions, and twin activities as functions of Y concentration. The plasticity analysis revealed that increasing Y decreases the CRSS ratio of prismatic and pyramidal slip relative to TT1 twinning, while the slip-to-twin CRSS ratio for TT2 increases, thereby serving as a potential indicator of differential twin activity with Y addition in Mg alloys. Additionally, despite their small volume fraction, TT2 twin sites were predicted higher local strain accumulation locally, relative to the representative volume element and TT1 twins, suggesting their potential influence on localized phenomena such as recrystallization or twin nucleation. These findings provide insight into local mechanical behavior in Mg alloys and support alloy design for advanced engineering applications.</p>
  <a class="paper-link" href="https://arxiv.org/pdf/2602.16632v1" target="_blank" rel="noopener noreferrer">
    View PDF →
  </a>
</article>

<article class="paper-card">
  <h3 class="paper-title">
    <a href="https://arxiv.org/pdf/2602.16628v1" target="_blank" rel="noopener noreferrer">
      Stoichiometry Dependent Properties of Cerium Hydride: An Active Learning Developed Interatomic Potential Study
    </a>
  </h3>
  <p class="paper-meta">
    <span class="paper-authors">Brenden W. Hamilton, Travis E. Jones, Timothy C. Germann, Benjamin T. Nebgen</span>
    <span class="paper-dot">•</span>
    <span class="paper-date">2026-02-18</span>
  </p>
  <p class="paper-abstract">Cerium hydride has a variety of interesting properties, including a known lattice contraction and densification with increasing hydrogen content. However, precise stoichiometric control is not experimentally straightforward and {\it ab initio} approaches are not computationally feasible for many properties such as melting and low temperature diffusion. Therefore, we develop a machine-learned interatomic potential for cerium hydride that is valid for H to Ce ratios from 2.0 to 3.0. A query-by-committee active learning approach is used to develop the training set. Leveraging classical molecular dynamics simulations, we assess a range of properties and provide fundamental mechanisms for the trends with stoichiometry. A majority of the properties follow the trend of lattice contraction, being governed by the stronger lattice binding induced by adding octahedral atoms.</p>
  <a class="paper-link" href="https://arxiv.org/pdf/2602.16628v1" target="_blank" rel="noopener noreferrer">
    View PDF →
  </a>
</article>

<article class="paper-card">
  <h3 class="paper-title">
    <a href="https://arxiv.org/pdf/2602.16618v1" target="_blank" rel="noopener noreferrer">
      Addressing Ill-conditioning in Density Functional Theory for Reliable Machine Learning
    </a>
  </h3>
  <p class="paper-meta">
    <span class="paper-authors">L. Arnstein, J. Wetherell, R. Lawrence, P. J. Hasnip, M. J. P. Hodgson</span>
    <span class="paper-dot">•</span>
    <span class="paper-date">2026-02-18</span>
  </p>
  <p class="paper-abstract">In principle, machine learning (ML) can be used to obtain any electronic property of a many-body system from its electron density within density functional theory. However, some physical quantities are highly sensitive to small variations in the density. This &#x27;ill-conditioning&#x27; limits the accuracy with which these quantities can be learned as density functionals from a fixed amount of data. We identify sources of ill-conditioning present in density functionals that belong to two ubiquitous classes: 1) Physical quantities that are globally gauge-dependent, meaning they change value if a constant shift is applied to the external potential -- for example, the total energy; 2) Functionals of the N-electron density that have an implicit dependence on the (N+1)-electron density, such as the fundamental gap. We demonstrate that widely used ML models exhibit orders-of-magnitude greater error when applied to these ill-conditioned density functionals compared to other functionals that fall into neither class, even when the global gauge is fixed to prevent constant shifts. Owing to an absence of ill-conditioning in potential functionals, we find that providing the external potential as input to the ML model leads to significantly improved predictions of quantities in these two classes.</p>
  <a class="paper-link" href="https://arxiv.org/pdf/2602.16618v1" target="_blank" rel="noopener noreferrer">
    View PDF →
  </a>
</article>

<article class="paper-card">
  <h3 class="paper-title">
    <a href="https://arxiv.org/pdf/2602.16614v1" target="_blank" rel="noopener noreferrer">
      Meteor statistics I: The distribution of instrumental magnitudes
    </a>
  </h3>
  <p class="paper-meta">
    <span class="paper-authors">Althea V. Moorhead, Peter G. Brown, Margaret D. Campbell-Brown, Michael J. Mazur, Denis Vida</span>
    <span class="paper-dot">•</span>
    <span class="paper-date">2026-02-18</span>
  </p>
  <p class="paper-abstract">The distribution of meteor magnitudes is known to follow an exponential distribution, where the base of this distribution is called the population index. The distribution of observed magnitudes preserves this behavior, but is truncated by the detection threshold. If both the population index and detection threshold can be determined, observed meteor rates can be converted to fluxes and extrapolated to any desired brightness or size. We argue that the distribution of observed or instrumental meteor magnitudes is best modeled as an exponentially modified Gaussian (exGaussian) distribution. This is for three reasons: first, an exGaussian distribution is the natural result of random variations in detection threshold and/or post-detection measurement errors in magnitude. Second, an exGaussian distribution provides a better fit to the magnitude distribution than all other competing distributions in the literature; we demonstrate this using both a set of faint optical meteor magnitudes and a set of radar meteor echo amplitudes. Finally, the population index, mean detection threshold, and random variation/error terms are easily extracted from the best-fit parameters of an exGaussian distribution.</p>
  <a class="paper-link" href="https://arxiv.org/pdf/2602.16614v1" target="_blank" rel="noopener noreferrer">
    View PDF →
  </a>
</article>

<article class="paper-card">
  <h3 class="paper-title">
    <a href="https://arxiv.org/pdf/2602.16612v1" target="_blank" rel="noopener noreferrer">
      Causal and Compositional Abstraction
    </a>
  </h3>
  <p class="paper-meta">
    <span class="paper-authors">Robin Lorenz, Sean Tull</span>
    <span class="paper-dot">•</span>
    <span class="paper-date">2026-02-18</span>
  </p>
  <p class="paper-abstract">Abstracting from a low level to a more explanatory high level of description, and ideally while preserving causal structure, is fundamental to scientific practice, to causal inference problems, and to robust, efficient and interpretable AI. We present a general account of abstractions between low and high level models as natural transformations, focusing on the case of causal models. This provides a new formalisation of causal abstraction, unifying several notions in the literature, including constructive causal abstraction, Q-$τ$ consistency, abstractions based on interchange interventions, and `distributed&#x27; causal abstractions. Our approach is formalised in terms of category theory, and uses the general notion of a compositional model with a given set of queries and semantics in a monoidal, cd- or Markov category; causal models and their queries such as interventions being special cases. We identify two basic notions of abstraction: downward abstractions mapping queries from high to low level; and upward abstractions, mapping concrete queries such as Do-interventions from low to high. Although usually presented as the latter, we show how common causal abstractions may, more fundamentally, be understood in terms of the former. Our approach also leads us to consider a new stronger notion of `component-level&#x27; abstraction, applying to the individual components of a model. In particular, this yields a novel, strengthened form of constructive causal abstraction at the mechanism-level, for which we prove characterisation results. Finally, we show that abstraction can be generalised to further compositional models, including those with a quantum semantics implemented by quantum circuits, and we take first steps in exploring abstractions between quantum compositional circuit models and high-level classical causal models as a means to explainable quantum AI.</p>
  <a class="paper-link" href="https://arxiv.org/pdf/2602.16612v1" target="_blank" rel="noopener noreferrer">
    View PDF →
  </a>
</article>

<article class="paper-card">
  <h3 class="paper-title">
    <a href="https://arxiv.org/pdf/2602.16601v1" target="_blank" rel="noopener noreferrer">
      Error Propagation and Model Collapse in Diffusion Models: A Theoretical Study
    </a>
  </h3>
  <p class="paper-meta">
    <span class="paper-authors">Nail B. Khelifa, Richard E. Turner, Ramji Venkataramanan</span>
    <span class="paper-dot">•</span>
    <span class="paper-date">2026-02-18</span>
  </p>
  <p class="paper-abstract">Machine learning models are increasingly trained or fine-tuned on synthetic data. Recursively training on such data has been observed to significantly degrade performance in a wide range of tasks, often characterized by a progressive drift away from the target distribution. In this work, we theoretically analyze this phenomenon in the setting of score-based diffusion models. For a realistic pipeline where each training round uses a combination of synthetic data and fresh samples from the target distribution, we obtain upper and lower bounds on the accumulated divergence between the generated and target distributions. This allows us to characterize different regimes of drift, depending on the score estimation error and the proportion of fresh data used in each generation. We also provide empirical results on synthetic data and images to illustrate the theory.</p>
  <a class="paper-link" href="https://arxiv.org/pdf/2602.16601v1" target="_blank" rel="noopener noreferrer">
    View PDF →
  </a>
</article>

<article class="paper-card">
  <h3 class="paper-title">
    <a href="https://arxiv.org/pdf/2602.16600v1" target="_blank" rel="noopener noreferrer">
      Predicting The Cop Number Using Machine Learning
    </a>
  </h3>
  <p class="paper-meta">
    <span class="paper-authors">Meagan Mann, Christian Muise, Erin Meger</span>
    <span class="paper-dot">•</span>
    <span class="paper-date">2026-02-18</span>
  </p>
  <p class="paper-abstract">Cops and Robbers is a pursuit evasion game played on a graph, first introduced independently by Quilliot \cite{quilliot1978jeux} and Nowakowski and Winkler \cite{NOWAKOWSKI1983235} over four decades ago. A main interest in recent the literature is identifying the cop number of graph families. The cop number of a graph, $c(G)$, is defined as the minimum number of cops required to guarantee capture of the robber. Determining the cop number is computationally difficult and exact algorithms for this are typically restricted to small graph families. This paper investigates whether classical machine learning methods and graph neural networks can accurately predict a graph&#x27;s cop number from its structural properties and identify which properties most strongly influence this prediction. Of the classical machine learning models, tree-based models achieve high accuracy in prediction despite class imbalance, whereas graph neural networks achieve comparable results without explicit feature engineering. The interpretability analysis shows that the most predictive features are related to node connectivity, clustering, clique structure, and width parameters, which aligns with known theoretical results. Our findings suggest that machine learning approaches can be used in complement with existing cop number algorithms by offering scalable approximations where computation is infeasible.</p>
  <a class="paper-link" href="https://arxiv.org/pdf/2602.16600v1" target="_blank" rel="noopener noreferrer">
    View PDF →
  </a>
</article>

<article class="paper-card">
  <h3 class="paper-title">
    <a href="https://arxiv.org/pdf/2602.16568v1" target="_blank" rel="noopener noreferrer">
      Separating Oblivious and Adaptive Models of Variable Selection
    </a>
  </h3>
  <p class="paper-meta">
    <span class="paper-authors">Ziyun Chen, Jerry Li, Kevin Tian, Yusong Zhu</span>
    <span class="paper-dot">•</span>
    <span class="paper-date">2026-02-18</span>
  </p>
  <p class="paper-abstract">Sparse recovery is among the most well-studied problems in learning theory and high-dimensional statistics. In this work, we investigate the statistical and computational landscapes of sparse recovery with $\ell_\infty$ error guarantees. This variant of the problem is motivated by \emph{variable selection} tasks, where the goal is to estimate the support of a $k$-sparse signal in $\mathbb{R}^d$. Our main contribution is a provable separation between the \emph{oblivious} (``for each&#x27;&#x27;) and \emph{adaptive} (``for all&#x27;&#x27;) models of $\ell_\infty$ sparse recovery. We show that under an oblivious model, the optimal $\ell_\infty$ error is attainable in near-linear time with $\approx k\log d$ samples, whereas in an adaptive model, $\gtrsim k^2$ samples are necessary for any algorithm to achieve this bound. This establishes a surprising contrast with the standard $\ell_2$ setting, where $\approx k \log d$ samples suffice even for adaptive sparse recovery. We conclude with a preliminary examination of a \emph{partially-adaptive} model, where we show nontrivial variable selection guarantees are possible with $\approx k\log d$ measurements.</p>
  <a class="paper-link" href="https://arxiv.org/pdf/2602.16568v1" target="_blank" rel="noopener noreferrer">
    View PDF →
  </a>
</article>
          </div>
        </div>
      </section>
    </main>

    <footer class="site-footer">
      <div class="container footer-content">
        <p>© <span id="year"></span> Tim's Coding Blog · arXiv feed</p>
      </div>
    </footer>

    <script>
      document.getElementById("year").textContent =
        new Date().getFullYear().toString();
    </script>
  </body>
  </html>

